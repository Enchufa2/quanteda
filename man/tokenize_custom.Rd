% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizers.R
\docType{data}
\name{tokenize_custom}
\alias{tokenize_custom}
\alias{data_breakrules}
\title{Customizable tokenizer}
\format{
\code{data_breakrules} is a list of pre-define rules. \code{word} is copied
from the ICU library. Other rules are created by the package maintainers.
\describe{
\item{\code{word}}{ICU's rules for detecting word boundaries}
\item{\code{sent}}{ICU's rules for detecting sentece boundaries}
\item{\code{hyphen}}{quanteda's rule for preserving hyphens}
\item{\code{url}}{quanteda's rule for preserving URLs}
\item{\code{email}}{quanteda's rule for preserving emails}
\item{\code{elision}}{quanteda's rule for splitting at elisions}
}
}
\source{
\url{https://raw.githubusercontent.com/unicode-org/icu/main/icu4c/source/data/brkitr/rules/word.txt}
}
\usage{
tokenize_custom(x, rules)

data_breakrules
}
\description{
Allows users to tokenize texts using customized boundary rules. See the \href{https://unicode-org.github.io/icu/userguide/boundaryanalysis/break-rules.html}{ICU website}
for how to define boundary rules.
}
\examples{
tokenize_custom("a well-known website http://example.com", data_breakrules["word"])
tokenize_custom("a well-known http://example.com", data_breakrules)
}
\keyword{data}
