% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizers.R
\docType{data}
\name{tokenize_custom}
\alias{tokenize_custom}
\alias{data_breakrules}
\title{Customizable tokenizer}
\format{
\code{data_breakrules} is a list of pre-define rules. \code{word} is copied
from the ICU library. Other rules are created by the package maintainers.
\describe{
\item{\code{word}}{default rules for tokenizing words}
\item{\code{hyphens}}{custom rule for preserving hyphens}
\item{\code{url}}{custom rule for preserving URLs}
\item{\code{email}}{custom rule for preserving emails}
\item{\code{elision}}{custom rule for splitting at elisions}
}
}
\source{
\url{https://raw.githubusercontent.com/unicode-org/icu/main/icu4c/source/data/brkitr/rules/word.txt}
}
\usage{
tokenize_custom(x, rules)

data_breakrules
}
\description{
Allows users to tokenize texts using customized boundary rules. See the \href{https://unicode-org.github.io/icu/userguide/boundaryanalysis/break-rules.html}{ICU website}
for how to define boundary rules.
}
\examples{
tokenize_custom("a cool website #hashtag http://example.com", data_breakrules["word"])
tokenize_custom("a cool website #hashtag http://example.com", data_breakrules)
}
\keyword{data}
